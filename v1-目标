v1 目标
1. 跨平台库适配
实现Linux和Windows系统的兼容适配

确保所有依赖库在两个平台都能正常运行


支持批量数据处理

4. 数据预处理
实现数据归一化方法

支持多种归一化策略

5. 数据集管理
实现数据分集功能

支持训练集、验证集、测试集的划分

6. 训练框架
实现DDP（分布式数据并行）训练

实现单卡训练模式

支持训练过程交互控制

7. 训练流程优化
实现训练主体代码精简

提高代码可读性和可维护性

8. 结果与参数保存
实现训练结果保存

实现模型参数保存

支持训练状态恢复

9. 可视化与前端
实现初步的训练可视化

实现前端界面展示训练结果

基础架构
./
├── docs/                    # 文档
├── data/                    # 原始数据
├── logs/                    # 日志（按日期组织）
│   ├── 2024-01-30/
│   │   ├── exp1-model1/
│   │   └── exp2-model2/
├── src/                     # 源代码
│   ├── data/               # 数据处理模块
│   ├── model/              # 模型定义
├── scripts/                # 脚本文件
├── test/                   # 测试代码
├── tmp/                    # 临时文件
└── requirements.txt        # 依赖包



加载模块

加载配置
    配置类
    记录类
加载数据
    模式选择(测试模式，正常模式)
数据处理
    嵌入（onehot,plm;tokenize)

    标准化(标签)

数据分集
    dataloader
    比例分/标签分
训练
    模式选择(单卡；回归,分类；超参数；amp；损失函数；)
    指标评估
    指标记录

结果记录
    配置
    训练过程
    模型网络
    模型权重
    结果张量



bug
目前saver不会根据state和result改jobs_info.json
目前无法取消实验

todo
compute的设计仍未完成。需要好好设计一下。

完善compute,在本机测试。

连接compute和master




def build_iupac_vocab(n_cls=2):
    """
    构建IUPAC氨基酸词表，确保每个token有唯一的递增ID
    
    参数:
        n_cls (int): <cls> 标记的数量，默认为2
    
    返回:
        OrderedDict: 词表映射字典，格式为 {token: unique_incrementing_id}
    """
    vocab = OrderedDict()
    current_id = 0
    
    # 1. 基础特殊标记
    vocab["<pad>"] = current_id
    current_id += 1
    
    vocab["<mask>"] = current_id
    current_id += 1
    
    # 2. 添加多个 <cls> 标记
    for i in range(1, n_cls + 1):
        vocab[f"<cls{i}>"] = current_id
        current_id += 1
    
    # 3. 其他特殊标记
    vocab["<sep>"] = current_id
    current_id += 1
    
    vocab["<unk>"] = current_id
    current_id += 1
    
    vocab["<sos>"] = current_id
    current_id += 1
    
    vocab["<eos>"] = current_id
    current_id += 1
    
    # 4. 添加26个字母
    for letter in "ABCDEFGHIJKLMNOPQRSTUVWXYZ":
        vocab[letter] = current_id
        current_id += 1
    
    return vocab
